{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a4fd913",
   "metadata": {},
   "source": [
    "# Artificial neural network\n",
    "A MachineLearning algorithm that is roughly modelled around  what is currently known about how the human brain  functions.\n",
    "![](NN_img_1.png)\n",
    "![](NN_img_2.png)\n",
    "![](NN_img_4.png)\n",
    "![](NN-img_3.png)\n",
    "\n",
    "# Neural network architechture\n",
    "Made of layers with many interconnected nodes(neurons). There are three main layers, specifically\n",
    "1. Input Layer\n",
    "2. Hidden Layer\n",
    "3. OutputLayer\n",
    "\n",
    "Hidden Layer can be one ormore\n",
    "![](NN_img_5.png)\n",
    "![](NN_img_7.png)\n",
    "\n",
    "1. Input nodes process the incoming data  exactly asreceived\n",
    "2. Network has only one set of connection  weights(w1, w2, andw3)\n",
    "3. It is therefore termed a single-layer  network\n",
    "\n",
    "![](NN_img_9.png)\n",
    "    Adds one or more hidden  layersthat process the signalsfrom  the input nodes prior to  reaching the output node\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fafb34",
   "metadata": {},
   "source": [
    "# Backpropagation:-\n",
    "Backpropagation is the essence of neural network training. It is the method of fine-tuning the weights of a neural network based on the error rate obtained in the previous iteration. Proper tuning of the weights allows you to reduce error rates and make the model reliable by increasing its generalization.\n",
    "\n",
    "Backpropagation in neural network is a short form for “backward propagation of errors.” It is a standard method of training artificial neural networks. This method helps calculate the gradient of a loss function with respect to all the weights in the network.ation).\n",
    "\n",
    "![](NN_img_10.png)\n",
    "\n",
    "1. The output node gives a predicted  value\n",
    "2. The difference between predicted  value and actual value is the error\n",
    "3. Error propagated backward byapportioning them to each node's weights\n",
    "4. In proportion to the amount of this error the node is responsible for"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d709570",
   "metadata": {},
   "source": [
    "# Basic Learning Rules in the Neural Network:-\n",
    "Learning rule or Learning process is a method or a mathematical logic. It improves the Artificial Neural Network’s performance and applies this rule over the network. Thus learning rules updates the weights and bias levels of a network when a network simulates in a specific data environment.\n",
    "Applying learning rule is an iterative process. It helps a neural network to learn from the existing conditions and improve its performance.\n",
    "\n",
    "Let us see different learning rules in the Neural network:\n",
    "\n",
    "1. Hebbian learning rule – It identifies, how to modify the weights of nodes of a network.\n",
    "2. Perceptron learning rule – Network starts its learning by assigning a random value to each weight.\n",
    "3. Delta learning rule – Modification in sympatric weight of a node is equal to the multiplication of error and the input.\n",
    "4. Correlation learning rule – The correlation rule is the supervised learning.\n",
    "5. Outstar learning rule – We can use it when it assumes that nodes or neurons in a network arranged in a layer.\n",
    "\n",
    "# 1. Hebbian learning rule \n",
    "The Hebb learning rule assumes that – If two neighbor neurons activated and deactivated at the same time. Then the weight connecting these neurons should increase. For neurons operating in the opposite phase, the weight between them should decrease. If there is no signal correlation, the weight should not change.\n",
    "When inputs of both the nodes are either positive or negative, then a strong positive weight exists between the nodes. If the input of a node is positive and negative for other, a strong negative weight exists between the nodes.\n",
    "![](NN_img_11.png)\n",
    "\n",
    "# 2. Perceptron learning rule\n",
    "each connection in a neural network has an associated weight, which changes in the course of learning. According to it, an example of supervised learning, the network starts its learning by assigning a random value to each weight.\n",
    "Calculate the output value on the basis of a set of records for which we can know the expected output value. This is the learning sample that indicates the entire definition. As a result, it is called a learning sample.\n",
    "![](NN_img_12.png)\n",
    "\n",
    "# 3. Delta learning rule\n",
    "This rule states that the modification in sympatric weight of a node is equal to the multiplication of error and the input.\n",
    "In Mathematical form the delta rule is as follows:\n",
    "![](NN_img_13.png)\n",
    "\n",
    "# 4. Correlation learning rule\n",
    "The correlation learning rule based on a similar principle as the Hebbian learning rule. It assumes that weights between responding neurons should be more positive, and weights between neurons with opposite reaction should be more negative.\n",
    "\n",
    "Contrary to the Hebbian rule, the correlation rule is the supervised learning. Instead of an actual\n",
    "The response, oj, the desired response, dj, uses for the weight-change calculation.\n",
    "In Mathematical form the correlation learning rule is as follows:\n",
    "![](NN_img_14.png)\n",
    "\n",
    "# 5. Outstar learning rule\n",
    "We use the Out Star Learning Rule when we assume that nodes or neurons in a network arranged in a layer. Here the weights connected to a certain node should be equal to the desired outputs for the neurons connected through those weights. The out start rule produces the desired response t for the layer of n nodes.\n",
    "\n",
    "Apply this type of learning for all nodes in a particular layer. Update the weights for nodes are as in Kohonen neural networks.\n",
    "In Mathematical form, express the out star learning as follows:\n",
    "![](NN_img_15.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5993af0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
