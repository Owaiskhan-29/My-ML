{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Coefficient:\n",
    "\n",
    "    Correlation coefficients measure the strength of association between two variables. The most common correlation coefficient, called the Pearson product-moment correlation coefficient, measures the strength of the linear association between variables measured on an interval or ratio scale.\n",
    "\n",
    "    when we speak simply of a correlation coefficient, we are referring to the Pearson product-moment correlation. Generally, the correlation coefficient of a sample is denoted by r, and the correlation coefficient of a population is denoted by ρ (Roh).\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to Interpret a Correlation Coefficient\n",
    "\n",
    "    The sign and the absolute value of a correlation coefficient describe the direction and the magnitude of the relationship between two variables.\n",
    "    \n",
    "    (The absolute value of a number refers to the magnitude of the number, without regard to its sign. The absolute value of -1 and 1 is 1, the absolute value of -2 and 2 is 2, the absolute value of -3 and 3 is 3, and so on. )\n",
    "    \n",
    "    > The value of a correlation coefficient ranges between -1 and 1.\n",
    "    > The greater the absolute value of the Pearson product-moment correlation coefficient, the stronger the linear relationship.\n",
    "    > The strongest linear relationship is indicated by a correlation coefficient of -1 or 1.\n",
    "    > The weakest linear relationship is indicated by a correlation coefficient equal to 0.\n",
    "    > A positive correlation means that if one variable gets bigger, the other variable tends to get bigger.\n",
    "    > A negative correlation means that if one variable gets bigger, the other variable tends to get smaller.\n",
    "    \n",
    "    \n",
    "    Keep in mind that the Pearson product-moment correlation coefficient only measures linear relationships. Therefore, a correlation of 0 does not mean zero relationship between two variables; rather, it means zero linear relationship. (It is possible for two variables to have zero linear relationship and a strong curvilinear relationship at the same time.)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatterplots and Correlation Coefficients\n",
    "\n",
    "    The scatterplots below show how different patterns of data produce different degrees of correlation.\n",
    "    \n",
    "   ![](img1.png)\n",
    "   \n",
    "       Several points are evident from the scatterplots.\n",
    "       \n",
    "       > When the slope of the line in the plot is negative, the correlation is negative; and vice versa.\n",
    "       > The strongest correlations (r = 1.0 and r = -1.0 ) occur when data points fall exactly on a straight line\n",
    "       > The correlation becomes weaker as the data points become more scattered.\n",
    "       > If the data points fall in a random pattern, the correlation is equal to zero.\n",
    "       > Correlation is affected by outliers. Compare the first scatterplot with the last scatterplot. The single outlier in the last plot greatly reduces the correlation (from 1.00 to 0.71).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression:\n",
    "\n",
    "    In a cause and effect relationship, the independent variable is the cause, and the dependent variable is the effect. Least squares linear regression is a method for predicting the value of a dependent variable Y, based on the value of an independent variable X.\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prerequisites for Regression\n",
    "\n",
    "    Simple linear regression is appropriate when the following conditions are satisfied.\n",
    "    \n",
    "    > The dependent variable Y has a linear relationship to the independent variable X. To check this, make sure that the XY scatterplot is linear and that the residual plot shows a random pattern.\n",
    "    \n",
    "    > For each value of X, the probability distribution of Y has the same standard deviation σ. When this condition is satisfied, the variability of the residuals will be relatively constant across all values of X, which is easily checked in a residual plot.\n",
    "    \n",
    "    > For any given value of X,\n",
    "            > The Y values are independent, as indicated by a random pattern on the residual plot.\n",
    "            > The Y values are roughly normally distributed (i.e., symmetric and unimodal). A little skewness is ok if the sample size is large. A histogram or a dotplot will show the shape of the distribution.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Least Squares Regression Line\n",
    "\n",
    "    Linear regression finds the straight line, called the least squares regression line or LSRL, that best represents observations in a bivariate data set. Suppose Y is a dependent variable, and X is an independent variable. The population regression line is:\n",
    "    \n",
    "                            y = mx + b\n",
    "                            \n",
    "                                or, some books written as \n",
    "                               \n",
    "                            y = b0 + b1x (same as above)\n",
    "                            \n",
    "          where b0 is a constant, b1 is the regression coefficient, X is the value of the independent variable, and Y is the value of the dependent variable.\n",
    "          \n",
    "          \n",
    "                                \n",
    "                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to Define a Regression Line\n",
    "\n",
    "    b1 = Σ [ (xi - x̅)(yi - y̅) ] / Σ [ (xi - x̅)2]\n",
    "    \n",
    "    b1 = r * (sy / sx)\n",
    "\n",
    "    b0 = y̅ - b1 * x̅\n",
    "    \n",
    "    where b0 is the constant in the regression equation, b1 is the regression coefficient, r is the correlation between x and y, xi is the X value of observation i, yi is the Y value of observation i, x is the mean of X, y is the mean of Y, sx is the standard deviation of X, and sy is the standard deviation of Y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Properties of the Regression Line\n",
    "\n",
    "    When the regression parameters (b0 and b1) are defined as described above, the regression line has the following properties.\n",
    "    \n",
    "    > The line minimizes the sum of squared differences between observed values (the y values) and predicted values (the ŷ values computed from the regression equation).\n",
    "    \n",
    "    > The regression line passes through the mean of the X values (x) and through the mean of the Y values (y).\n",
    "    \n",
    "    > The regression constant (b0) is equal to the y intercept of the regression line.\n",
    "    \n",
    "    > The regression coefficient (b1) is the average change in the dependent variable (Y) for a 1-unit change in the independent variable (X). It is the slope of the regression line.\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Coefficient of Determination\n",
    "\n",
    "    The coefficient of determination (denoted by R²) is a key output of regression analysis. It is interpreted as the proportion of the variance in the dependent variable that is predictable from the independent variable.\n",
    "    \n",
    "    > The coefficient of determination ranges from 0 to 1.\n",
    "    \n",
    "    > An R² of 0 means that the dependent variable cannot be predicted from the independent variable.\n",
    "    \n",
    "    > An R² of 1 means the dependent variable can be predicted without error from the independent variable.\n",
    "    \n",
    "    > An R² between 0 and 1 indicates the extent to which the dependent variable is predictable. An R² of 0.10 means that 10 percent of the variance in Y is predictable from X; an R² of 0.20 means that 20 percent is predictable; and so on.\n",
    "    \n",
    "    If you know the linear correlation (r) between two variables, then the coefficient of determination (R²) is easily computed using the following formula: R2 = r2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Error\n",
    "\n",
    "    The standard error about the regression line (often denoted by SE) is a measure of the average amount that the regression equation over- or under-predicts. The higher the coefficient of determination, the lower the standard error; and the more accurate predictions are likely to be.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Stretch/Untouched | ProbDistribution | Accuracy |\n",
    "| --- | --- | --- |\n",
    "| Stretched | Gaussian | .843 |\n",
    "|+++++|+++++| --- |\n",
    "| Stretched | Gaussian | .843 |\n",
    "| --- | --- | --- |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fundamental of Statistics\n",
    "\n",
    "\n",
    "| <h3>Function</h3> | <h3>Symbol</h3> | <h3>Formulae</h3> | <h3>Description</h3> |\n",
    "| --- | --- | --- | --- |\n",
    "| <h3>Average/Mean</h3> | <h3>$\\large \\mu \\space or \\space \\bar {x}$</h3> | <h3>$\\large \\frac {\\sum_{i = 0}^n {x_i}} {n}$</h3> | <h3>Average value of a Series; n = number of observations</h3> |\n",
    "| <hr></hr> | <hr></hr> | <hr></hr> | <hr></h3> |\n",
    "| <h3>Variance</h3> | <h3>$\\large s^2 \\space or \\space \\sigma^2$</h3> | <h4>$\\large \\frac {\\sum_{i \\space = \\space 1}^{n}(x_i \\space - \\bar{x})^2} {n \\space - \\space 1}$</h4> | <h3>Large Spread of the data from the Mean</h3> |\n",
    "| <hr></hr> | <hr></hr> | <hr></hr> | <hr></h3> |\n",
    "| <h3>Std. Deviation</h3> | <h3>$\\large s \\space or \\space \\sigma$</h3> | <h4>$\\large \\sqrt {\\frac {\\sum_{i \\space = \\space 1}^{n}(x_i \\space - \\bar{x})^2} {n \\space - \\space 1}}$</h4> | <h3>Spread of the data from the Mean</h3>\n",
    "| <hr></hr> | <hr></hr> | <hr></hr> | <hr></h3> |\n",
    "| <h3>Z Score or Standardized Score</h3> | <h3>$\\large z$</h3> | <h3>$\\large \\frac {x \\space - \\space \\bar {x}} {s}$</h3> | <h3>How many standard deviations away from the Mean</h3>\n",
    "| <hr></hr> | <hr></hr> | <hr></hr> | <hr></h3> |\n",
    "| <h3>Skew</h3> | <h3>$\\large sk$</h3> | <h3>$\\large \\frac {\\sum_{i \\space = \\space 1}^{n}(z_i)^3} {n \\space - \\space 1}$</h3> | <h4>- 0 means distribution is symmetric - Usually a score between -1 and +1 - Positive sk indicates +ve skewed data - Negative sk indicates –ve skewed data </h4> |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
